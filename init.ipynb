{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038eadf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 12\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7172e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heeyyy\n"
     ]
    }
   ],
   "source": [
    "print(\"heeyyy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1757632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.13/site-packages (1.102.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.13/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230a717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525562b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"hey\"\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308a5ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.74 (from langchain_openai)\n",
      "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in ./.venv/lib/python3.13/site-packages (from langchain_openai) (1.102.0)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.11.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting langsmith>=0.3.45 (from langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading langsmith-0.4.20-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading orjson-3.11.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai)\n",
      "  Downloading zstandard-0.24.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai)\n",
      "  Downloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
      "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.11.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading langsmith-0.4.20-py3-none-any.whl (377 kB)\n",
      "Downloading orjson-3.11.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m759.5/759.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.24.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, urllib3, tenacity, regex, PyYAML, orjson, jsonpointer, charset_normalizer, requests, jsonpatch, tiktoken, requests-toolbelt, langsmith, langchain-core, langchain_openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [langchain_openai][langchain-core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 charset_normalizer-3.4.3 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.75 langchain_openai-0.3.32 langsmith-0.4.20 orjson-3.11.3 regex-2025.7.34 requests-2.32.5 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.11.0 urllib3-2.5.0 zstandard-0.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e11b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-openai\n",
      "Version: 0.3.32\n",
      "Summary: An integration package connecting OpenAI and LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/Musaed/Documents/GitHub_My-Repos/Task1/.venv/lib/python3.13/site-packages\n",
      "Requires: langchain-core, openai, tiktoken\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bd376",
   "metadata": {},
   "source": [
    "## Testing The OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df3942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LangChain OpenAI wrapper\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Simple call\n",
    "response = llm.invoke(\"hey\")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To connect Twitter to Lucidya, follow these steps:\n",
      "\n",
      "1. **Log in to Lucidya**.\n",
      "2. **Go to Settings**.\n",
      "3. **Select Integrations**.\n",
      "4. **Find Twitter**.\n",
      "5. **Click on Connect**.\n",
      "6. **Authorize Lucidya** to access your Twitter account.\n",
      "\n",
      "If you encounter any issues, check your Twitter account permissions. \n",
      "\n",
      "Next, try connecting and let me know if you need further help!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ---- Strategy A prompts ----\n",
    "SYSTEM = (\n",
    "    \"You are a helpful Lucidya product assistant. Detect the user’s language and respond \"\n",
    "    \"in the same language (Arabic ↔ English). Prioritize correctness, simplicity, and actionable steps. \"\n",
    "    \"If unsure, ask one clarifying question before answering. Never invent features; if information is missing, \"\n",
    "    \"say so and suggest what to check.\"\n",
    ")\n",
    "\n",
    "DEV = (\n",
    "    \"Assistant Instruction:\\n\"\n",
    "    \"Use short sentences. Prefer lists. Provide at most one example. \"\n",
    "    \"End with a brief next step. If Arabic, write in Modern Standard Arabic.\"\n",
    ")\n",
    "\n",
    "def generate_response(end_user_query: str, kb_chunks: str | None = None) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"system\", \"content\": DEV},\n",
    "    ]\n",
    "    if kb_chunks and kb_chunks.strip():\n",
    "        messages.append({\"role\": \"system\", \"content\": \"[Lucidya KB snippets]\\n\" + kb_chunks})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": end_user_query})\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "        temperature=float(os.getenv(\"OPENAI_TEMPERATURE\", \"0.2\")),\n",
    "        messages=messages,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate_response(\"How do I connect Twitter to Lucidya?\"))\n",
    "    # kb = \"- Twitter integration: Settings > Channels > Add Twitter.\\n- Requires app keys and callback URL.\\n- Sync delay: ~5 min.\"\n",
    "    # print(generate_response(\"كيف أربط تويتر بحسابي؟\", kb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd3a4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Prompt sent to model -----\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a helpful Lucidya product assistant. Detect the user’s language and respond in the same language (Arabic ↔ English). Prioritize correctness, simplicity, and actionable steps. If unsure, ask one clarifying question before answering. Never invent features; if information is missing, say so and suggest what to check.Assistant Instruction:\\nUse short sentences. Prefer lists. Provide at most one example. End with a brief next step. If Arabic, write in Modern Standard Arabic.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"How do I connect Twitter to Lucidya?\"\n",
      "  }\n",
      "]\n",
      "\n",
      "----- Response -----\n",
      "To connect Twitter to Lucidya, follow these steps:\n",
      "\n",
      "1. Log in to your Lucidya account.\n",
      "2. Go to the \"Integrations\" section.\n",
      "3. Find Twitter in the list of available integrations.\n",
      "4. Click on \"Connect\" or \"Add\".\n",
      "5. Authorize Lucidya to access your Twitter account.\n",
      "6. Follow any additional prompts to complete the setup.\n",
      "\n",
      "Next, check your Twitter settings to ensure permissions are granted.\n",
      "\n",
      "----- Trace Info -----\n",
      "ID: chatcmpl-C9dgsBEZmK9q65CM1BFzTlTsFoHSD\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Prompt tokens: 118\n",
      "Completion tokens: 92\n",
      "Total tokens: 210\n",
      "Elapsed time (s): 3.681\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ---- Strategy A prompts ----\n",
    "SYSTEM = (\n",
    "    \"You are a helpful Lucidya product assistant. Detect the user’s language and respond \"\n",
    "    \"in the same language (Arabic ↔ English). Prioritize correctness, simplicity, and actionable steps. \"\n",
    "    \"If unsure, ask one clarifying question before answering. Never invent features; if information is missing, \"\n",
    "    \"say so and suggest what to check.\"\n",
    ")\n",
    "\n",
    "DEV = (\n",
    "    \"Assistant Instruction:\\n\"\n",
    "    \"Use short sentences. Prefer lists. Provide at most one example. \"\n",
    "    \"End with a brief next step. If Arabic, write in Modern Standard Arabic.\"\n",
    ")\n",
    "\n",
    "def generate_response(end_user_query: str, kb_chunks: str | None = None) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"system\", \"content\": DEV},\n",
    "    ]\n",
    "    if kb_chunks and kb_chunks.strip():\n",
    "        messages.append({\"role\": \"system\", \"content\": \"[Lucidya KB snippets]\\n\" + kb_chunks})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": end_user_query})\n",
    "\n",
    "    # Log the full prompt\n",
    "    print(\"----- Prompt sent to model -----\")\n",
    "    print(json.dumps(messages, indent=2, ensure_ascii=False))\n",
    "\n",
    "    start = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "        temperature=float(os.getenv(\"OPENAI_TEMPERATURE\", \"0.2\")),\n",
    "        messages=messages,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    text = resp.choices[0].message.content\n",
    "    usage = resp.usage\n",
    "\n",
    "    print(\"\\n----- Response -----\")\n",
    "    print(text)\n",
    "    print(\"\\n----- Trace Info -----\")\n",
    "    print(\"ID:\", resp.id)\n",
    "    print(\"Model:\", resp.model)\n",
    "    print(\"Prompt tokens:\", usage.prompt_tokens)\n",
    "    print(\"Completion tokens:\", usage.completion_tokens)\n",
    "    print(\"Total tokens:\", usage.total_tokens)\n",
    "    print(\"Elapsed time (s):\", round(elapsed, 3))\n",
    "\n",
    "    return text  # you can use it programmatically if needed\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    generate_response(\"How do I connect Twitter to Lucidya?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5b378b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Prompt sent to model -----\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a helpful Lucidya product assistant. Detect the user’s language and respond in the same language (Arabic ↔ English). Prioritize correctness, simplicity, and actionable steps. If unsure, ask one clarifying question before answering. Never invent features; if information is missing, say so and suggest what to check.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"Assistant Instruction:\\nUse short sentences. Prefer lists. Provide at most one example. End with a brief next step. If Arabic, write in Modern Standard Arabic.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"How do I connect Twitter to Lucidya?\"\n",
      "  }\n",
      "]\n",
      "\n",
      "----- Response -----\n",
      "To connect Twitter to Lucidya, follow these steps:\n",
      "\n",
      "1. Log in to your Lucidya account.\n",
      "2. Go to the \"Integrations\" section.\n",
      "3. Find Twitter in the list of available integrations.\n",
      "4. Click on \"Connect\" or \"Add.\"\n",
      "5. Authorize Lucidya to access your Twitter account.\n",
      "6. Follow any additional prompts to complete the connection.\n",
      "\n",
      "Next, check if your Twitter feed is displaying correctly in Lucidya.\n",
      "\n",
      "----- Trace Info -----\n",
      "ID: chatcmpl-C9ddyX1QcydGAcUFFe9YESMC6vbbr\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Prompt tokens: 122\n",
      "Completion tokens: 95\n",
      "Total tokens: 217\n",
      "Elapsed time (s): 2.426\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ec742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42bf60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Prompt sent to model -----\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a helpful Lucidya product assistant. Detect the user’s language and respond in the same language (Arabic ↔ English). Prioritize correctness, simplicity, and actionable steps. If unsure, ask one clarifying question before answering. Never invent features; if information is missing, say so and suggest what to check.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"Assistant Instruction:\\nUse short sentences. Prefer lists. Provide at most one example. End with a brief next step. If Arabic, write in Modern Standard Arabic.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"How do I connect Twitter to Lucidya?\"\n",
      "  }\n",
      "]\n",
      "\n",
      "----- Response -----\n",
      "To connect Twitter to Lucidya, follow these steps:\n",
      "\n",
      "1. Log in to your Lucidya account.\n",
      "2. Go to the \"Integrations\" section.\n",
      "3. Select \"Twitter\" from the list of available integrations.\n",
      "4. Click on \"Connect\" or \"Authorize.\"\n",
      "5. Log in to your Twitter account if prompted.\n",
      "6. Allow Lucidya to access your Twitter data.\n",
      "\n",
      "Next, check if the connection is successful by viewing your Twitter data in Lucidya.\n",
      "\n",
      "----- Trace Info -----\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Prompt tokens: 122\n",
      "Completion tokens: 99\n",
      "Total tokens: 221\n",
      "Elapsed time (s): 4.579\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ---- Strategy A prompts ----\n",
    "SYSTEM = (\n",
    "    \"You are a helpful Lucidya product assistant. Detect the user’s language and respond \"\n",
    "    \"in the same language (Arabic ↔ English). Prioritize correctness, simplicity, and actionable steps. \"\n",
    "    \"If unsure, ask one clarifying question before answering. Never invent features; if information is missing, \"\n",
    "    \"say so and suggest what to check.\"\n",
    ")\n",
    "\n",
    "DEV = (\n",
    "    \"Assistant Instruction:\\n\"\n",
    "    \"Use short sentences. Prefer lists. Provide at most one example. \"\n",
    "    \"End with a brief next step. If Arabic, write in Modern Standard Arabic.\"\n",
    ")\n",
    "\n",
    "def generate_response(end_user_query: str, kb_chunks: str | None = None) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"system\", \"content\": DEV},\n",
    "    ]\n",
    "    if kb_chunks and kb_chunks.strip():\n",
    "        messages.append({\"role\": \"system\", \"content\": \"[Lucidya KB snippets]\\n\" + kb_chunks})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": end_user_query})\n",
    "\n",
    "    # Log the full prompt\n",
    "    print(\"----- Prompt sent to model -----\")\n",
    "    print(json.dumps(messages, indent=2, ensure_ascii=False))\n",
    "\n",
    "    start = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "        temperature=float(os.getenv(\"OPENAI_TEMPERATURE\", \"0.2\")),\n",
    "        messages=messages,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    text = resp.choices[0].message.content\n",
    "    usage = resp.usage\n",
    "\n",
    "    print(\"\\n----- Response -----\")\n",
    "    print(text)\n",
    "    print(\"\\n----- Trace Info -----\")\n",
    "    print(\"Model:\", resp.model)\n",
    "    print(\"Prompt tokens:\", usage.prompt_tokens)\n",
    "    print(\"Completion tokens:\", usage.completion_tokens)\n",
    "    print(\"Total tokens:\", usage.total_tokens)\n",
    "    print(\"Elapsed time (s):\", round(elapsed, 3))\n",
    "\n",
    "    return text  # you can use it programmatically if needed\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    generate_response(\"How do I connect Twitter to Lucidya?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a0915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
